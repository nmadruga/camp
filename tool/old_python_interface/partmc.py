#!/usr/bin/env python
# Copyright (C) 2007-2012, 2016 Matthew West
# Licensed under the GNU General Public License version 2 or (at your
# option) any later version. See the file COPYING for details.

import os
import sys
import re
import textwrap
import math
import numpy
import random
import scipy
import scipy.io
import scipy.optimize

def vol2rad(volumes):
    return (volumes * 3.0/4.0 / math.pi)**(1.0/3.0)

def rad2vol(radii):
    return 4.0/3.0 * math.pi * radii**3

def diam2rad(diameters):
    return diameters / 2.0

def rad2diam(radii):
    return radii * 2.0

def vol2diam(volumes):
    return rad2diam(vol2rad(volumes))

def diam2vol(diameters):
    return rad2vol(diam2rad(diameters))

def rad2surf(radii):
    return 4.0 * math.pi * radii**2

def surf2rad(surface_areas):
    return (surface_areas / 4.0 / math.pi)**0.5

def diam2surf(diameters):
    return rad2surf(diam2rad(diameters))

def surf2diam(surface_areas):
    return rad2diam(surf2rad(surface_areas))

def vol2surf(volumes):
    return rad2surf(vol2rad(volumes))

def surf2vol(surface_areas):
    return rad2vol(surf2rad(surface_areas))

class constants_t(object):
    """Autogenerated from constants.f90."""
    pi = 3.14159265358979323846e0
    boltzmann = 1.3806505e-23
    avagadro = 6.02214179e23
    univ_gas_const = 8.314472e0
    accom_coeff = 1e0
    water_eq_vap_press = 611e0
    water_freeze_temp = 273.15e0
    water_surf_eng = 0.073e0
    water_latent_heat = 2.272e6
    air_spec_heat = 1005e0
    water_molec_weight = 18e-3
    water_density = 1e3
    air_molec_weight = 2.89644e-2
    air_std_press = 101325e0
    air_dyn_visc = 1.78e-5

def constants_f2py(constants_f90_filename):
    
    """Converts constants.f90 from the PartMC source to a Python
    class.

    Example:
    >>> camp.constants_t('path_to_src/constants.f90')

    """
    consts_file = open(constants_f90_filename)
    in_const_t = False
    found_const_t = False
    start_re = re.compile("^ *type const_t *$")
    end_re = re.compile("^ *end type const_t *$")
    const_re = re.compile("^ *real[(]kind=dp[)] :: ([^ ]+) = ([-0-9.]+)d([-0-9]+) *$")
    print("class constants_t(object):")
    print("    \"\"\"Autogenerated from constants.f90.\"\"\"")
    for line in consts_file:
        if in_const_t:
            match = const_re.search(line)
            if match:
                name = match.group(1)
                mantissa = match.group(2)
                exponent = match.group(3)
                print("    %s = %se%s" % (name, mantissa, exponent))
            if end_re.search(line):
                in_const_t = False
        else:
            if start_re.search(line):
                in_const_t = True
                found_const_t = True
    if not found_const_t:
        raise Exception("constants.f90 ended without finding const_t")
    if in_const_t:
        raise Exception("constants.f90 ended without finding end of const_t")

def _get_netcdf_variable_data(nv):

    """Extracts the data in a netcdf_variable object and returns it as
    a numpy ndarray, a scalar, or a similar object."""

    return nv[()]

class aero_data_t(object):

    """Stores the physical constants describing aerosol species. All
    data atributes are 1D arrays, with one entry per species. Thus
    names[i] and densities[i] give the name and density of the i-th
    aerosol species, respectively.

    All data attribute arrays have the same length, so the number of
    species can be found, for example, with:
    
    >>> n_aero_species = len(aero_data.names)

    The data attributes are:
    
    names - names of the aerosol species (strings)
    mosaic_indices - species index numbers in MOSAIC (integers)
    densities - density of each species (kg/m^3)
    num_ions - number of ions for dissociation (integers)
    molec_weights - molecular weights (kg/mole)
    kappas - hydroscopicity parameters (dimensionless)
    source_names - names of the particle sources (strings)

    There is one class data member, species_tex_names, that is a
    dictionary mapping ASCII species names to LaTeX species names.

    Example:
    >>> species_name = "SO4"
    >>> aero_data_t.species_tex_names[species_name]
    "SO$_4$"

    """

    species_tex_names = {
        "SO4": r"SO$_4$",
        "NO3": r"NO$_3$",
        "Cl": r"Cl",
        "NH4": r"NH$_4$",
        "MSA": r"MSA",
        "ARO1": r"ARO1",
        "ARO2": r"ARO2",
        "ALK1": r"ALK1",
        "OLE1": r"OLE1",
        "API1": r"API1",
        "API2": r"API2",
        "LIM1": r"LIM1",
        "LIM2": r"LIM2",
        "CO3": r"CO$_3$",
        "Na": r"Na",
        "Ca": r"Ca",
        "OIN": r"OIN",
        "OC": r"POA",
        "BC": r"BC",
        "H2O": r"H$_2$O",
        }

    def __init__(self, ncf=None, n_species=None, n_sources=None):
        """Creates an aero_data_t object. If the ncf parameter is
        passed a netcdf_file object output from PartMC then the
        aero_data information will be loaded from the file. Otherwise
        the n_species parameter must be passed and an empty
        aero_data_t object will be created. For example:

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> aero_data = camp.aero_data_t(ncf)

        or

        >>> aero_data = camp.aero_data_t(n_species=10, n_sources=3)

        """
        if ncf is not None:
            if "aero_species" not in ncf.variables.keys():
                raise Exception("aero_species variable not found in NetCDF file")
            if "names" not in dir(ncf.variables["aero_species"]):
                raise Exception("aero_species variable does not have 'names' attribute")
            self.names = ncf.variables["aero_species"].names.split(",")

            if "aero_source" not in ncf.variables.keys():
                raise Exception("aero_source variable not found in NetCDF file")
            if "names" not in dir(ncf.variables["aero_source"]):
                raise Exception("aero_source variable does not have 'names' attribute")
            self.source_names = ncf.variables["aero_source"].names.split(",")

            for (ncf_var, self_var) in [
                ("aero_mosaic_index", "mosaic_indices"),
                ("aero_density", "densities"),
                ("aero_num_ions", "num_ions"),
                ("aero_molec_weight", "molec_weights"),
                ("aero_kappa", "kappas"),
                ]:
                if ncf_var not in ncf.variables.keys():
                    raise Exception("%s variable not found in NetCDF file" % ncf_var)
                self.__dict__[self_var] = _get_netcdf_variable_data(ncf.variables[ncf_var])
                
        if n_species is not None:
            if ncf is not None:
                raise Exception("ncf and n_species arguments cannot both be specified")
            if n_sources is None:
                raise Exception("n_species and n_sources arguments must be specified together")
            self.names = ["" for i in range(n_species)]
            self.mosaic_indices = numpy.zeros([n_species], float)
            self.densities = numpy.zeros([n_species], float)
            self.num_ions = numpy.zeros([n_species], int)
            self.molec_weights = numpy.zeros([n_species], float)
            self.kappas = numpy.zeros([n_species], float)
            self.names = ["" for i in range(n_species)]
        else:
            if n_sources is not None:
                raise Exception("n_species and n_sources arguments must be specified together")

        if ncf is None and n_species is None:
            raise Exception("either ncf or n_species parameter must be specified")

    def index_list(self, include=None, exclude=None):
        """Find the indices of the species specified by the given
        optional include and exclude lists.

        aero_data.index_list() uses all species.
        aero_data.index_list(include=...) uses all species in the include list.
        aero_data.index_list(exclude=...) uses all species not in the exclude list.
        aero_data.index_list(include=..., exclude=...) uses all species in the
        include list that are not in the exclude list.

        Example:
        >>> aero_data.names
        ['SO4', 'NO3', 'BC', 'OC', 'H2O']
        >>> aero_data.index_list()
        [0, 1, 2, 3, 4]
        >>> aero_data.index_list(include=['BC', 'OC'])
        [2, 3]
        >>> aero_data.index_list(exclude=['NO3', 'H2O'])
        [0, 2, 3]
        >>> aero_data.index_list(include=['NO3', 'BC', 'OC'], exclude=['BC'])
        [1, 3]
        """
        if include != None:
            for species in include:
                if species not in self.names:
                    raise Exception("unknown species: %s" % species)
            species_list = set(include)
        else:
            species_list = set(self.names)
        if exclude != None:
            for species in exclude:
                if species not in self.names:
                    raise Exception("unknown species: %s" % species)
            species_list -= set(exclude)
        species_list = list(species_list)
        if len(species_list) == 0:
            raise Exception("no species in list")
        index_list = [self.names.index(s) for s in species_list]
        return index_list

class env_state_t(object):

    """Stores the environment state. All data attributes are scalars,
    giving the state of the variable at a single point in time and
    space. The data attributes are:

    temperature - air temperature (K)
    relative_humidity - air relative humidity (dimensionless)
    pressure - air pressure (Pa)
    longitude - location longitude (degrees)
    latitude - location latitude (degrees)
    altitude - location altitude (m)
    start_time_of_day - time of day when simulation started (s)
    start_day_of_year - day number of year when the simulation
        started (integer)
    elapsed_time - elapsed simulation time since the simulation start (s)
    solar_zenith_angle - angle between the zenith and the sun (radians)
    height - mixing layer height (m)
    

    """
    
    def __init__(self, ncf=None):
        """Creates an env_state_t object. If the ncf parameter is
        passed a netcdf_file object output from PartMC then the
        env_state information will be loaded from the file. Otherwise
        an empty env_state_t object will be created. For example:

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> env_state = camp.env_state_t(ncf)

        or

        >>> env_state = camp.env_state_t()

        """
        if ncf is not None:
            for (ncf_var, self_var) in [
                ("temperature", "temperature"),
                ("relative_humidity", "relative_humidity"),
                ("pressure", "pressure"),
                ("longitude", "longitude"),
                ("latitude", "latitude"),
                ("altitude", "altitude"),
                ("start_time_of_day", "start_time_of_day"),
                ("start_day_of_year", "start_day_of_year"),
                ("elapsed_time", "elapsed_time"),
                ("solar_zenith_angle", "solar_zenith_angle"),
                ("height", "height"),
                ]:
                if ncf_var not in ncf.variables.keys():
                    raise Exception("%s variable not found in NetCDF file" % ncf_var)
                self.__dict__[self_var] = _get_netcdf_variable_data(ncf.variables[ncf_var])
        else:
            self.temperature = 0.0
            self.relative_humidity = 0.0
            self.pressure = 0.0
            self.longitude = 0.0
            self.latitude = 0.0
            self.altitude = 0.0
            self.start_time_of_day = 0.0
            self.start_day_of_year = 0
            self.elapsed_time = 0.0
            self.solar_zenith_angle = 0.0
            self.height = 0.0

    def A(self):
        """Computes the A parameter (for condensation
        calculations). For example:

        >>> A = env_state.A()
        
        """
        return (4.0 * constants_t.water_surf_eng * constants_t.water_molec_weight
                / (constants_t.univ_gas_const * self.temperature *
                   constants_t.water_density))

class gas_data_t(object):

    """Stores the physical constants describing gas species. All data
    atributes are 1D arrays, with one entry per species. Thus names[i]
    gives of the i-th gas species.

    All data attribute arrays have the same length, so the number of
    species can be found, for example, with:
    
    >>> n_gas_species = len(gas_data.names)

    The data attributes are:
    
    names - names of the gas species (strings)
    mosaic_indices - species index numbers in MOSAIC (integers)

    There is one class data member, species_tex_names, that is a
    dictionary mapping ASCII species names to LaTeX species names.

    Example:
    >>> species_name = "H2SO4"
    >>> aero_data_t.species_tex_names[species_name]
    "H$_2$SO$_4$"

    """

    species_tex_names = {
        "H2SO4": r"H$_2$SO$_4$",
        "HNO3": r"HNO$_3$",
        "HCl": r"HC$\ell$",
        "NH3": r"NH$_3$",
        "NO": r"NO",
        "NO2": r"NO$_2$",
        "NO3": r"NO$_3$",
        "N2O5": r"N$_2$O$_5$",
        "HONO": r"HONO",
        "HNO4": r"HNO$_4$",
        "O3": r"O$_3$",
        "O1D": r"O$_1$D",
        "O3P": r"O$_3$P",
        "OH": r"OH",
        "HO2": r"HO$_2$",
        "H2O2": r"H$_2$O$_2$",
        "CO": r"CO",
        "SO2": r"SO$_2$",
        "CH4": r"CH$_4$",
        "C2H6": r"C$_2$H$_6$",
        "CH3O2": r"CH$_3$O$_2$",
        "ETHP": r"ETHP",
        "HCHO": r"HCHO",
        "CH3OH": r"CH$_3$OH",
        "ANOL": r"ANOL",
        "CH3OOH": r"CH$_3$OOH",
        "ETHOOH": r"ETHOOH",
        "ALD2": r"ALD2",
        "HCOOH": r"HCOOH",
        "RCOOH": r"RCOOH",
        "C2O3": r"C$_2$O$_3$",
        "PAN": r"PAN",
        "ARO1": r"ARO1",
        "ARO2": r"ARO2",
        "ALK1": r"ALK1",
        "OLE1": r"OLE1",
        "API1": r"API1",
        "API2": r"API2",
        "LIM1": r"LIM1",
        "LIM2": r"LIM2",
        "PAR": r"PAR",
        "AONE": r"AONE",
        "MGLY": r"MGLY",
        "ETH": r"ETH",
        "OLET": r"OLET",
        "OLEI": r"OLEI",
        "TOL": r"TOL",
        "XYL": r"XYL",
        "CRES": r"CRES",
        "TO2": r"TO2",
        "CRO": r"CRO",
        "OPEN": r"OPEN",
        "ONIT": r"ONIT",
        "ROOH": r"ROOH",
        "RO2": r"RO2",
        "ANO2": r"ANO2",
        "NAP": r"NAP",
        "XO2": r"XO2",
        "XPAR": r"XPAR",
        "ISOP": r"ISOP",
        "ISOPRD": r"ISOPRD",
        "ISOPP": r"ISOPP",
        "ISOPN": r"ISOPN",
        "ISOPO2": r"ISOPO2",
        "API": r"API",
        "LIM": r"LIM",
        "DMS": r"DMS",
        "MSA": r"MSA",
        "DMSO": r"DMSO",
        "DMSO2": r"DMSO2",
        "CH3SO2H": r"CH$_3$SO$_2$H",
        "CH3SCH2OO": r"CH$_3$SCH$_2$OO",
        "CH3SO2": r"CH$_3$SO$_2$",
        "CH3SO3": r"CH$_3$SO$_3$",
        "CH3SO2OO": r"CH$_3$SO$_2$OO",
        "CH3SO2CH2OO": r"CH$_3$SO$_2$CH$_2$OO",
        "SULFHOX": r"SULFHOX",
        }
    
    def __init__(self, ncf=None, n_species=None):
        """Creates a gas_data_t object. If the ncf parameter is
        passed a netcdf_file object output from PartMC then the
        gas_data information will be loaded from the file. Otherwise
        the n_species parameter must be passed and an empty gas_data_t
        object will be created. For example:

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> gas_data = camp.gas_data_t(ncf)

        or

        >>> gas_data = camp.gas_data_t(n_species=60)

        """
        if ncf is not None:
            if "gas_species" not in ncf.variables.keys():
                raise Exception("gas_species variable not found in NetCDF file")
            if "names" not in dir(ncf.variables["gas_species"]):
                raise Exception("gas_species variable does not have 'names' attribute")
            self.names = ncf.variables["gas_species"].names.split(",")

            for (ncf_var, self_var) in [
                ("gas_mosaic_index", "mosaic_indices"),
                ]:
                if ncf_var not in ncf.variables.keys():
                    raise Exception("%s variable not found in NetCDF file" % ncf_var)
                self.__dict__[self_var] = _get_netcdf_variable_data(ncf.variables[ncf_var])
                
        if n_species is not None:
            if ncf is not None:
                raise Exception("ncf and n_species arguments cannot both be specified")
            self.names = ["" for i in range(n_species)]
            self.mosaic_indices = numpy.zeros([n_species], float)

        if ncf is None and n_species is None:
            raise Exception("either ncf or n_species parameter must be specified")

class gas_state_t(object):

    """Stores the gas state at a single point in time and space. The
    data attributes are:

    raw_mixing_ratios - mixing ratios of each gas species (dimensionless)
    gas_data - object of type gas_data_t with per-species physical data

    The mixing ratio of a gas species should be obtained by using the
    mixing_ratio() method, not by accessing the raw_mixing_ratios
    attribute directly, to avoid confusion with species numbers. For
    example:

    >>> ozone = gas_state.mixing_ratio('O3')

    """
    
    def __init__(self, ncf=None, gas_data=None):
        """Creates a gas_state_t object. If the ncf parameter is
        passed a netcdf_file object output from PartMC then the
        gas_state information will be loaded from the file. Otherwise
        the gas_data parameter must be passed and an empty gas_state
        will be created with the appropriate number of species. For
        example:

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> gas_state = camp.gas_state_t(ncf)

        or

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> gas_data = camp.gas_data_t(ncf)
        >>> gas_state = camp.gas_state_t(gas_data=gas_data)

        """
        if ncf is not None:
            if gas_data is not None:
                raise Exception("cannot specify both ncf and gas_data parameters")
            self.gas_data = gas_data_t(ncf)
            if "gas_mixing_ratio" not in ncf.variables.keys():
                raise Exception("gas_mixing_ratio variable not found in NetCDF file")
            self.raw_mixing_ratios = _get_netcdf_variable_data(ncf.variables["gas_mixing_ratio"])
        else:
            if gas_data is None:
                raise Exception("must specify either ncf or gas_data parameters")
            self.gas_data = gas_data
            self.raw_mixing_ratios = numpy.zeros([len(gas_data.names)], float)

    def mixing_ratio(self, species):
        """Return the mixing ratio of the given species name. For
        example, the mixing ratio of ozone is given by:

        >>> ozone = gas_state.mixing_ratio('O3')

        """
        if species not in self.gas_data.names:
            raise Exception("unknown species: %s" % species)
        index = self.gas_data.names.index(species)
        return self.raw_mixing_ratios[index]

class aero_particle_array_t(object):

    """Stores the particles at a single point in time and space. All
    data attributes (except aero_data) are 1D or 2D arrays with one
    entry (or column if 2D) per aerosol particle. The data attributes
    are (assuming S species and N particles):

    aero_data - object of type aero_data_t with per-species physical data
    raw_masses - SPEC x N array of the mass (kg) of species i in particle j
    n_orig_parts - SOURCE x N array with the number of original particles
        from source i that now compose particle j
    weight_groups - length N array with weight group numbers of each
        particle
    weight_sets - length N array with weight set numbers of each particle
    absorb_cross_sects - length N array with absorbion cross section (m^2
        of each particle
    scatter_cross_sects - length N array with scattering cross section
        (m^2) of each particle
    asymmetries - length N array with asymmetry parameter of each particle
    refract_shell_reals - length N array with the real part of the
        refractive index of the shell of each particle
    refract_shell_imags - length N array with the imaginary part of the
        refractive index of the shell of each particle
    refract_core_reals - length N array with the real part of the
        refractive index of the core of each particle
    refract_core_imags - length N array with the imaginary part of the
        refractive index of the core of each particle
    core_vols - length N array with the volume of the core (m^3) of each
        particle
    water_hyst_legs - length N array with which leg of the water
        hysteresis curve the particle is on
    num_concs - length N array with the number concentration (m^{-3})
        associated with each particle
    ids - length N array with the ID number of each particle
    least_create_times - length N array with the earliest creation
        time (s) of any component of each particle
    greatest_create_times - length N array with the latest creation
        time (s) of any component of each particle

    Most of these attributes should be accessed directly, except for
    the raw_masses array. To prevent confusion, this should be
    accessed by the masses() method.

    To get the number of particles we can use, for example:
    >>> n_particles = len(aero_particle_array.ids)

    """
    
    def __init__(self, ncf=None, n_particles=None, aero_data=None,
                 include_ids=None, exclude_ids=None):
        """Creates an aero_particle_array_t object. If the ncf
        parameter is passed a netcdf_file object output from PartMC
        then the aero_particle_array information will be loaded from
        the file. The include_ids and exclude_ids parameters can be
        given lists of particle IDs to include or exclude when loading
        from the file.

        If the ncf parameter is not specified then the aero_data and
        n_particles parameters must be given and an empty
        aero_particle_array_t will be created with the appropriate
        number of particles.

        For example:

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> aero_particle_array = camp.aero_particle_array_t(ncf)

        or

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> aero_particle_array = camp.aero_particle_array_t(ncf,
                include_ids=[45, 182, 7281])

        or

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> aero_data = camp.aero_data_t(ncf)
        >>> aero_particle_array \\
                = camp.aero_particle_array_t(n_particles=100,
                                               aero_data=aero_data)

        """
        if ncf == None:
            if aero_data is None:
                raise Exception("must pass aero_data when ncf parameter is not given")
            if n_particles is None:
                raise Exception("must pass n_particles when ncf parameter is not given")
            if include_ids is not None:
                raise Exception("cannot provide include_ids without the ncf parameter")
            if exclude_ids is not None:
                raise Exception("cannot provide include_ids without the ncf parameter")
            self.aero_data = aero_data
            self.raw_masses = zeros([len(aero_data.names), n_particles])
            self.n_orig_parts = zeros([len(aero_data.source_names), n_particles], int)
            self.weight_groups = zeros(n_particles, int)
            self.weight_sets = zeros(n_particles, int)
            self.absorb_cross_sects = zeros(n_particles)
            self.scatter_cross_sects = zeros(n_particles)
            self.asymmetries = zeros(n_particles)
            self.refract_shell_reals = zeros(n_particles)
            self.refract_shell_imags = zeros(n_particles)
            self.refract_core_reals = zeros(n_particles)
            self.refract_core_imags = zeros(n_particles)
            self.core_vols = zeros(n_particles)
            self.water_hyst_legs = zeros(n_particles, int)
            self.num_concs = zeros(n_particles)
            self.ids = zeros(n_particles, int)
            self.least_create_times = zeros(n_particles)
            self.greatest_create_times = zeros(n_particles)
            return

        if aero_data is not None:
            raise Exception("cannot specify aero_data when ncf parameter is given")
        if n_particles is not None:
            raise Exception("cannot specify n_particles when ncf parameter is given")

        self.aero_data = aero_data_t(ncf)

        for (ncf_var, self_var, required) in [
            ("aero_particle_mass", "raw_masses", True),
            ("aero_n_orig_part", "n_orig_parts", True),
            ("aero_particle_weight_group", "weight_groups", False),
            ("aero_particle_weight_set", "weight_sets", False),
            ("aero_absorb_cross_sect", "absorb_cross_sects", False),
            ("aero_scatter_cross_sect", "scatter_cross_sects", False),
            ("aero_asymmetry", "asymmetries", False),
            ("aero_refract_shell_real", "refract_shell_reals", False),
            ("aero_refract_shell_imag", "refract_shell_imags", False),
            ("aero_refract_core_real", "refract_core_reals", False),
            ("aero_refract_core_imag", "refract_core_imags", False),
            ("aero_core_vol", "core_vols", False),
            ("aero_water_hyst_leg", "water_hyst_legs", True),
            ("aero_num_conc", "num_concs", True),
            ("aero_id", "ids", True),
            ("aero_least_create_time", "least_create_times", True),
            ("aero_greatest_create_time", "greatest_create_times", True),
            ]:
            if ncf_var in ncf.variables.keys():
                self.__dict__[self_var] = _get_netcdf_variable_data(ncf.variables[ncf_var])
            else:
                self.__dict__[self_var] = None
                if required:
                    raise Exception("%s variable not found in NetCDF file" % ncf_var)

        if include_ids != None or exclude_ids != None:
            keep_indexes = [i for i in range(size(self.ids)) \
                            if (include_ids != None and self.ids[i] in include_ids) \
                            or (exclude_ids != None and self.ids[i] not in exclude_ids)]
            self.raw_masses = self.raw_masses[:, keep_indexes]
            self.n_orig_parts = self.n_orig_parts[:, keep_indexes]
            if self.weight_groups is not None:
                self.weight_groups = self.weight_groups[keep_indexes]
            if self.weight_sets is not None:
                self.weight_sets = self.weight_sets[keep_indexes]
            if self.absorb_cross_sects is not None:
                self.absorb_cross_sects = self.absorb_cross_sects[keep_indexes]
            if self.scatter_cross_sects is not None:
                self.scatter_cross_sects = self.scatter_cross_sects[keep_indexes]
            if self.asymmetries is not None:
                self.asymmetries = self.asymmetries[keep_indexes]
            if self.refract_shell_reals is not None:
                self.refract_shell_reals = self.refract_shell_reals[keep_indexes]
            if self.refract_shell_imags is not None:
                self.refract_shell_imags = self.refract_shell_imags[keep_indexes]
            if self.refract_core_reals is not None:
                self.refract_core_reals = self.refract_core_reals[keep_indexes]
            if self.refract_core_imags is not None:
                self.refract_core_imags = self.refract_core_imags[keep_indexes]
            if self.core_vols is not None:
                self.core_vols = self.core_vols[keep_indexes]
            self.water_hyst_legs = self.water_hyst_legs[keep_indexes]
            self.num_concs = self.num_concs[keep_indexes]
            self.ids = self.ids[keep_indexes]
            self.least_create_times = self.least_create_times[keep_indexes]
            self.greatest_create_times = self.greatest_create_times[keep_indexes]

    def sort_by_id(self):
        """Sorts particles so that the IDs are in ascending order."""

        i = self.ids.argsort()
        self.raw_masses = self.raw_masses[:,i]
        self.n_orig_parts = self.n_orig_parts[:, i]
        if self.weight_groups is not None:
            self.weight_groups = self.weight_groups[i]
        if self.weight_sets is not None:
            self.weight_sets = self.weight_sets[i]
        if self.absorb_cross_sects is not None:
            self.absorb_cross_sects = self.absorb_cross_sects[i]
        if self.scatter_cross_sects is not None:
            self.scatter_cross_sects = self.scatter_cross_sects[i]
        if self.asymmetries is not None:
            self.asymmetries = self.asymmetries[i]
        if self.refract_shell_reals is not None:
            self.refract_shell_reals = self.refract_shell_reals[i]
        if self.refract_shell_imags is not None:
            self.refract_shell_imags = self.refract_shell_imags[i]
        if self.refract_core_reals is not None:
            self.refract_core_reals = self.refract_core_reals[i]
        if self.refract_core_imags is not None:           
            self.refract_core_imags = self.refract_core_imags[i]
        if self.core_vols is not None:
            self.core_vols = self.core_vols[i]
        self.water_hyst_legs = self.water_hyst_legs[i]
        self.num_concs = self.num_concs[i]
        self.ids = self.ids[i]
        self.least_create_times = self.least_create_times[i]
        self.greatest_create_times = self.greatest_create_times[i]

    def sum_masses_weighted(self, include=None, exclude=None,
                            species_weights=None):
        """Computes the weighted sum of the component masses of each
        particle, including and excluding the given species lists by
        name. This function should generally not be called directly,
        but instead the masses(), volumes(), etc. functions should be
        used.

        Example usage:
        >>> dry_volumes = aero_particle_array.sum_masses_weighted(
                exclude=['H2O'],
                species_weights=1 / aero_particle_array.aero_data.densities)

        """
        index_list = self.aero_data.index_list(include, exclude)
        val = numpy.zeros_like(self.raw_masses[0,:])
        for index in index_list:
            if species_weights != None:
                val += self.raw_masses[index,:] * species_weights[index]
            else:
                val += self.raw_masses[index,:]
        return val
    
    def masses(self, include=None, exclude=None):
        """Return the total mass (kg) of each particle as an array,
        including or excluding the given species by name. Examples:

        >>> total_masses = aero_particle_array.masses()
        >>> water_masses = aero_particle_array.masses(include=['H2O'])
        >>> dry_masses = aero_particle_array.masses(exclude=['H2O'])
        >>> carbon_masses = aero_particle_array.masses(include=['BC',
                'OC'])

        """
        return self.sum_masses_weighted(include=include, exclude=exclude)

    def dry_masses(self):
        """Return the dry mass (kg) of each particle as an array.

        """
        return self.masses(exclude=["H2O"])

    def volumes(self, include=None, exclude=None):
        """Return the total volume (m^3) of each particle as an array,
        including or excluding the given species by name. See the
        masses() method for examples of usage.

        """
        species_weights = 1.0 / self.aero_data.densities
        return self.sum_masses_weighted(include=include, exclude=exclude,
                                        species_weights=species_weights)

    def dry_volumes(self):
        """Return the dry volume (m^3) of each particle as an array.

        """
        return self.volumes(exclude=["H2O"])

    def moles(self, include=None, exclude=None):
        """Return the total moles (dimensionless) in each particle as
        an array, including or excluding the given species by
        name. See the masses() method for examples of usage.

        """
        species_weights = self.aero_data.molec_weights \
                          / self.aero_data.densities
        return self.sum_masses_weighted(include=include, exclude=exclude,
                                        species_weights=species_weights)

    def radii(self, include=None, exclude=None):
        """Return the radius (m) of each particle as an array,
        including or excluding the given species by name. See the
        masses() method for examples of usage.

        """
        return vol2rad(self.volumes(include=include, exclude=exclude))

    def dry_radii(self):
        """Return the dry radius (m) of each particle as an array.

        """
        return self.radii(exclude=["H2O"])

    def diameters(self, include=None, exclude=None):
        """Return the diameter (m) of each particle as an array,
        including or excluding the given species by name. See the
        masses() method for examples of usage.

        """
        return vol2diam(self.volumes(include=include, exclude=exclude))

    def dry_diameters(self):
        """Return the dry diameter (m) of each particle as an array.

        """
        return rad2diam(self.dry_radii())

    def surface_areas(self, include=None, exclude=None):
        """Return the surface area (m^2) of each particle as an array,
        including or excluding the given species by name. See the
        masses() method for examples of usage.

        """
        return vol2surf(self.volumes(include=include, exclude=exclude))

    def dry_surface_areas(self):
        """Return the dry surface area (m^2) of each particle as an array.

        """
        return self.surface_areas(exclude=["H2O"])

    def kappas(self):
        """Return the total kappa (dimensionless hydroscopicity
        parameter) of each particle as an array. This is computed as a
        volume-weighted sum of the per-species kappa values. Each
        species kappa can be specified directly in aero_data.kappa, or
        if this is zero it is calculated from aero_data.num_ions.

        """
        if "H2O" not in self.aero_data.names:
            raise Exception("unable to find water species index by name 'H2O'")
        i_water = self.aero_data.names.index("H2O")
        M_w = self.aero_data.molec_weights[i_water]
        rho_w = self.aero_data.densities[i_water]
        species_weights = numpy.zeros([len(self.aero_data.names)])
        for i_spec in range(len(species_weights)):
            if i_spec == self.aero_data.names == "H2O":
                continue
            if self.aero_data.num_ions[i_spec] > 0:
                if self.aero_data.kappas[i_spec] != 0:
                    raise Exception("species has nonzero num_ions and kappa: %s" % self.names[i_spec])
                M_a = self.aero_data.molec_weights[i_spec]
                rho_a = self.aero_data.densities[i_spec]
                species_weights[i_spec] = M_w * rho_a / (M_a * rho_w) \
                                          * self.aero_data.num_ions[i_spec]
            else:
                species_weights[i_spec] = self.aero_data.kappas[i_spec]
        species_weights /= self.aero_data.densities
        volume_kappas = self.sum_masses_weighted(exclude=["H2O"],
                                                 species_weights=species_weights)
        dry_volumes = self.volumes(exclude=["H2O"])
        return volume_kappas / dry_volumes

    def critical_rel_humids_approx(self, env_state):
        """Compute the critical relative humidities (dimensionless) of
        each particle as an array, using a fast approximate method.

        """
        A = env_state.A()
        C = sqrt(4.0 * A**3 / 27.0)
        dry_diameters = self.dry_diameters()
        kappas = self.kappas()
        S = C / sqrt(kappas * dry_diameters**3) + 1.0
        return S

    def critical_rel_humids(self, env_state):
        """Compute the critical relative humidities (dimensionless) of
        each particle as an array.

        """
        kappas = self.kappas()
        dry_diameters = self.dry_diameters()
        return critical_rel_humids(env_state, kappas, dry_diameters)

    def critical_diameters(self, env_state):
        """Compute the critical diameters (m) of each particle as an
        array.

        """
        kappas = self.kappas()
        dry_diameters = self.dry_diameters()
        return critical_diameters(env_state, kappas, dry_diameters)

    def equilib_rel_humids(self, env_state):
        """Compute the equilibrium relative humidities (dimensionless)
        of each particle as an array.

        """
        kappas = self.kappas()
        dry_diameters = self.dry_diameters()
        wet_diameters = self.diameters()
        equilib_rhs = numpy.zeros_like(wet_diameters)

        for i in range(len(wet_diameters)):
            equilib_rhs[i] = equilib_rel_humids(env_state, kappas[i], 
                                                dry_diameters[i], 
                                                numpy.array([wet_diameters[i]]))[0]
        return equilib_rhs

def equilib_rel_humids(env_state, kappa, dry_diameter, wet_diameters):
    """Compute the equilibrium relative humidity (dimensionless) for each
    wet_diameter.

    The kappa and dry_diameter parameters should be scalars, specifying a
    single particle. The wet_diameters parameter should be a 1D array of
    length N, and the return value will be a 1D array of length N with
    each entry i being the equilibrium RH for a particle with the given
    average kappa and the given dry_diameter.

    The env_state parameter should be an object of type env_state_t.

    Example:
    >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
    >>> env_state = camp.env_state_t(ncf)
    >>> kappa = 0.2
    >>> dry_diameter = 1e-8
    >>> wet_diameters = camp.log_grid(min=1.1e-8, max=1e-7, n_bin=20).edges()
    >>> equilib_rhs = camp.equilib_rel_humids(env_state,
            kappa, dry_diameter, wet_diameters)

    """
    A = env_state.A()
    e_rh = numpy.zeros_like(wet_diameters)
    for i in range(len(wet_diameters)):
        if kappa < 1e-30:
            e_rh[i] = numpy.exp(A / wet_diameters[i])
        else:
            e_rh[i] = (wet_diameters[i]**3 - dry_diameter**3) \
                / (wet_diameters[i]**3 - dry_diameter**3 * (1 - kappa)) \
                * numpy.exp(A / wet_diameters[i])
    return e_rh

def critical_rel_humids(env_state, kappas, dry_diameters):
    """Compute the critical relative humidity (dimensionless) for each
    kappa and dry_diameter.

    The kappas and dry_diameters parameters should be 1D arrays of the
    same length N, and the return value will be a 1D array of length N
    with each entry i being the critical RH for a particle with
    average kappa given by kappas[i] and a dry diameter of
    dry_diameters[i].

    The env_state parameter should be an object of type env_state_t.

    Example:
    >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
    >>> env_state = camp.env_state_t(ncf)
    >>> kappas = numpy.array([0.5, 0.2])
    >>> dry_diameters = numpy.array([1e-8, 5e-8])
    >>> crit_rhs = camp.critical_rel_humids(env_state,
            kappas, dry_diameters)

    """
    A = env_state.A()
    c_diams = critical_diameters(env_state, kappas, dry_diameters)
    c_rh = numpy.zeros_like(dry_diameters)
    for i in range(len(kappas)):
        if kappas[i] < 1e-30:
            c_rh[i] = numpy.exp(A / c_diams[i])
        else:
            c_rh[i] = (c_diams[i]**3 - dry_diameters[i]**3) \
                / (c_diams[i]**3 - dry_diameters[i]**3 * (1 - kappas[i])) \
                * numpy.exp(A / c_diams[i])
    return c_rh

def critical_diameters(env_state, kappas, dry_diameters):
    """Compute the critical diameters (m) for each kappa and
    dry_diameter.

    The kappas and dry_diameters parameters should be 1D arrays of the
    same length N, and the return value will be a 1D array of length N
    with each entry i being the critical diameter for a particle with
    average kappa given by kappas[i] and a dry diameter of
    dry_diameters[i].

    The env_state parameter should be an objects of type env_state_t.

    Example:
    >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
    >>> env_state = camp.env_state_t(ncf)
    >>> kappas = numpy.array([0.5, 0.2])
    >>> dry_diameters = numpy.array([1e-8, 5e-8])
    >>> crit_diams = camp.critical_diameters(env_state,
            kappas, dry_diameters)

    """
    A = env_state.A()
    c4 = - 3.0 * dry_diameters**3 * kappas / A
    c3 = - dry_diameters**3 * (2.0 - kappas)
    c0 = dry_diameters**6 * (1.0 - kappas)
    dc = numpy.zeros_like(dry_diameters)
    for i in range(len(kappas)):
        if kappas[i] < 1e-30:
            dc[i] = dry_diameters[i]
            continue
        def f(d):
            return d**6 + c4[i] * d**4 + c3[i] * d**3 + c0[i]
        d1 = dry_diameters[i]
        if not (f(d1) < 0):
            raise Exception("initialization failure for d1")
        d2 = 2 * d1
        for iteration in range(100):
            if f(d2) > 0:
                break
            d2 *= 2
        else:
            raise Exception("intialization failure for d2")
        dc[i] = scipy.optimize.brentq(f, d1, d2)
    return dc

class aero_removed_info_t(object):

    """Stores information about the particles removed from the aerosol
    particle population at a single point in time and space. All data
    attributes are arrays with one entry per removed aerosol
    particle. The data attributes are:

    ids - particle ID numbers of the removed particles
    actions - action code of each removed particle
    other_ids - associated particle ID numbers for the removals (or 0
        if there is no associated ID)

    The action codes are integers with the following values:
    aero_removed_info_t.AERO_INFO_NONE - No information.
    aero_removed_info_t.AERO_INFO_DILUTION - The particle was removed
        by diluting out of the parcel.
    aero_removed_info_t.AERO_INFO_COAG - The particle was removed due
        to coagulating with another particle. The ID of the particle
        coagulated with is given in the other_ids array.
    aero_removed_info_t.AERO_INFO_HALVED - The particle was removed
        due to a halving of the particle population.
    aero_removed_info_t.AERO_INFO_WEIGHT - The particle was removed
        due to re-weighting.

    To obtain the number of removed particles we can use, for example:
    >>> n_removed_particles = len(aero_removed_info.ids)

    Example:
    >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
    >>> aero_removed_info = camp.aero_removed_info_t(ncf)
    >>> n_removed_particles = len(aero_removed_info.ids)
    >>> for i in range(n_removed_particles):
    >>>     print('removed particle %d' % i)
    >>>     print('    id = %d' % aero_removed_info.ids[i])
    >>>     action = aero_removed_info.actions[i]
    >>>     if action == aero_removed_info.AERO_INFO_NONE:
    >>>         print('    no information')
    >>>     elif action == aero_removed_info.AERO_INFO_DILUTION:
    >>>         print('    removed due to dilution')
    >>>     elif action == aero_removed_info.AERO_INFO_COAG:
    >>>         print('    removed due to coagulation' \\)
    >>>             ' with particle id %d' \\
    >>>             % aero_removed_info.other_ids[i]
    >>>     elif action == aero_removed_info.AERO_INFO_HALVED:
    >>>         print('    removed due to halving')
    >>>     elif action == aero_removed_info.AERO_INFO_WEIGHT:
    >>>         print('    removed due to re-weighting')

    """
    
    AERO_INFO_NONE = 0
    AERO_INFO_DILUTION = 1
    AERO_INFO_COAG = 2
    AERO_INFO_HALVED = 3
    AERO_INFO_WEIGHT = 4

    def __init__(self, ncf):
        """Creates an aero_remove_info_t object by reading data from a
        NetCDF file output from PartMC.
        
        For example:
        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> aero_removed_info = camp.aero_removed_info_t(ncf)

        """
        for (ncf_var, self_var) in [
            ("aero_removed_id", "ids"),
            ("aero_removed_action", "actions"),
            ("aero_removed_other_id", "other_ids"),
            ]:
            if ncf_var not in ncf.variables.keys():
                raise Exception("%s variable not found in NetCDF file" % ncf_var)
            self.__dict__[self_var] = _get_netcdf_variable_data(ncf.variables[ncf_var])

        if (len(self.ids) == 1) and (self.ids[0] == 0):
            self.ids = numpy.array([], 'int32')
            self.actions = numpy.array([], 'int32')
            self.other_ids = numpy.array([], 'int32')

class aero_binned_t(object):

    """Stores number and mass distributions from a sectional PartMC
    run. The data attributes are:

    aero_data - object of type aero_data_t with per-species physical data
    diam_grid - object of type log_grid with diameter grid data
    number_conc - number concentration distribution (log_10)
    raw_mass_conc - mass concentration distribution per species (log_10)

    """

    def __init__(self, ncf):
        """Creates an aero_binned_t object. The ncf parameter must be
        a netcdf_file object output from PartMC.

        For example:

        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> aero_binned = camp.aero_binned_t(ncf)

        """
        self.aero_data = aero_data_t(ncf)
        self.diam_grid = log_grid()
        self.diam_grid.load_ncf_diam(ncf)

        for (ncf_var, self_var) in [
            ("aero_number_concentration", "num_conc"),
            ("aero_mass_concentration", "raw_mass_conc"),
            ]:
            if ncf_var in ncf.variables.keys():
                # convert (d/d ln D) to (d/d log10 D)
                self.__dict__[self_var] = numpy.log(10) \
                    * _get_netcdf_variable_data(ncf.variables[ncf_var])
            else:
                raise Exception("%s variable not found in NetCDF file" % ncf_var)

    def mass_conc(self, include=None, exclude=None):
        index_list = self.aero_data.index_list(include, exclude)
        val = numpy.zeros_like(self.raw_mass_conc[0,:])
        for index in index_list:
            val += self.raw_mass_conc[index,:]
        return val

class grid(object):

    """Base class for 1D grids. See camp.linear_grid and
    camp.log_grid for specific grid types.

    """
    
    def __init__(self):
        """Do not call this. Instead call camp.linear_grid() or
        camp.log_grid() to make a specific type of grid.

        """
        raise NotImplementedError()

    def find_clipped(self, values):
        """Find the bins for each entry of values, clipped to
        [0, n_bin - 1].

        The parameter values should be a 1D array of values to locate
        within the grid, and the return value is a 1D array of
        integers between 0 (the first bin) and n_bin - 1 (the last
        bin). If a value is below the first bin then 0 is returned
        while if it is above the last bin then n_bin - 1 is
        returned.

        """
        indices = self.find(values)
        indices = indices.clip(0, self.n_bin - 1)
        return indices

    def find_clipped_outer(self, values):
        """Find the bins for each entry of values, clipped to
        [-1, n_bin].

        The parameter values should be a 1D array of values to locate
        within the grid, and the return value is a 1D array of
        integers between -1 and n_bin. If each value is within a bin
        then a number within [0, n_bin - 1] is returned. If a value is
        below the first bin then -1 is returned while if it is above
        the last bin then n_bin is returned.

        """
        indices = self.find(values)
        indices = indices.clip(-1, self.n_bin)
        return indices

    def closest_edge(self, value):
        """Find the closest bin edge to the given value.

        Value should be a single scalar and the return value is an
        integer between 0 and n_bin giving the number of the edge
        closest to value.

        """
        i = self.find_clipped(value)
        lower_edge = self.edge(i)
        upper_edge = self.edge(i + 1)
        if abs(value - lower_edge) < abs(value - upper_edge):
            return i
        else:
            return i + 1

    def edges(self):
        """Return a length (n_bin + 1) array of the bin edges in the
        grid.

        """
        return numpy.array([self.edge(i) for i in range(self.n_bin + 1)])

    def centers(self):
        """Return a length n_bin array of the bin centers in the
        grid.

        """
        return numpy.array([self.center(i) for i in range(self.n_bin)])

class linear_grid(grid):

    """Linear 1D grid.

    Example:
    >>> x_grid = camp.linear_grid(0, 5, 100)
    >>> x = 1.83
    >>> print('value %f' % x)
    >>> print('is in bin number %d' % x_grid.find(x))

    """
    
    def __init__(self, min, max, n_bin):
        """Create a linearly spaced grid.

        The minimum and maximum edges are at min and max and the grid
        will have n_bin grid bins.

        Example:
        >>> x_grid = camp.linear_grid(0, 4, 2)
        >>> x_grid.edges()
        array([0.0, 2.0, 4.0])
        >>> x_grid.centers():
        array([1.0, 3.0])

        """
        self.min = float(min)
        self.max = float(max)
        if n_bin <= 0:
            raise Exception("n_bin must be positive for linear_grid")
        self.n_bin = n_bin

    def scale(self, factor):
        """Scale the grid by the given factor.

        For example, the two grids below are the same:

        >>> grid_1 = camp.linear_grid(5, 10, 100)
        >>> grid_1.scale(3)
        
        >>> grid_2 = camp.linear_grid(15, 30, 100)

        """
        self.min = self.min * factor
        self.max = self.max * factor

    def grid_size(self, index):
        """Return the size of the grid bin at the given index.

        For a linear grid this will be the same for all bins.

        Example:
        >>> x_grid = camp.linear_grid(0, 10, 5)
        >>> x_grid.grid_size(0)
        2.0

        """
        return (self.max - self.min) / float(self.n_bin)

    def valid_bin(self, bin):
        """Whether the given bin number is indeed a valid bin number
        for the grid.

        Example:
        >>> x_grid = camp.linear_grid(0, 5, 100)
        >>> x_grid.valid_bin(30)
        True
        >>> x_grid.valid_bin(120)
        False

        """
        if (bin >= 0) and (bin < self.n_bin):
            return True
        return False

    def find(self, values):
        """Return an array of bin indices corresponding to the given
        array of values.

        Note that invalid bin indices will be returned if any of the
        values are outside of [min, max] for the grid.

        Example:
        >>> x_grid = camp.linear_grid(0, 10, 5)
        >>> x_grid.find(numpy.array([-5, 3, 6.5, 13]))
        array([-3, 1, 3, 6])

        """
        indices = (numpy.floor((numpy.asarray(values) - self.min) * self.n_bin
                         / (self.max - self.min))).astype(int)
        return indices

    def edge(self, index):
        """Return the location of the bin edge at the given index.

        The index must be in the range 0 to n_bin, as a grid with
        n_bin grid cells will have (n_bin + 1) edges.

        Example:
        >>> x_grid = camp.linear_grid(0, 10, 5)
        >>> x_grid.edge(0)
        0.0
        >>> x_grid.edge(2)
        4.0
        >>> x_grid.edge(5)
        10.0

        """
        if (index < 0) or (index > self.n_bin):
            raise Exception("index out of range: %d" % index)
        if index == self.n_bin:
            return self.max
        elif index == 0:
            return self.min
        else:
            return float(index) / float(self.n_bin) * (self.max - self.min) \
                   + self.min

    def center(self, index):
        """Return the location of the bin center at the given
        index.

        The index must be in the range 0 to (n_bin - 1).

        Example:
        >>> x_grid = camp.linear_grid(0, 10, 5)
        >>> x_grid.center(0)
        1.0
        >>> x_grid.center(2)
        5.0
        >>> x_grid.center(4)
        9.0

        """
        if (index < 0) or (index >= self.n_bin):
            raise Exception("index out of range: %d" % index)
        return (float(index) + 0.5) / float(self.n_bin) \
               * (self.max - self.min) + self.min

    def half_sample(self):
        """Return a new linear_grid object with the same limits but
        half the number of bins as the current grid.

        """
        if self.n_bin % 2 != 0:
            raise Exception("n_bin must be an even number")
        return linear_grid(min=self.min, max=self.max,
                               n_bin=self.n_bin / 2)
        
class log_grid(grid):

    """Logarithmic 1D grid.

    Example:
    >>> x_grid = camp.log_grid(1, 5, 100)
    >>> x = 1.83
    >>> print('value %f' % x)
    >>> print('is in bin number %d' % x_grid.find(x))

    """
    
    def __init__(self, min=1, max=1, n_bin=1):
        """Create a logarithmically spaced grid.

        The minimum and maximum edges are at min and max and the grid
        will have n_bin grid bins.

        Example:
        >>> x_grid = camp.log_grid(1, 16, 2)
        >>> x_grid.edges()
        array([1.0, 4.0, 16.0])
        >>> x_grid.centers():
        array([2.0, 8.0])

        """
        if min <= 0 or max <= 0:
            raise Exception("min and max must both be positive for log_grid")
        self.min = float(min)
        self.max = float(max)
        if n_bin <= 0:
            raise Exception("n_bin must be positive for log_grid")
        self.n_bin = n_bin

    def load_ncf_diam(self, ncf):
        """Load the grid specification from the diameter grid in a
        NetCDF file containing a PartMC sectional output.

        Example:
        >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
        >>> diam_grid = camp.log_grid()
        >>> diam_grid.load_ncf_diam(ncf)

        """
        aero_diam_edges = _get_netcdf_variable_data(ncf.variables["aero_diam_edges"])
        self.min = aero_diam_edges[0]
        self.max = aero_diam_edges[-1]
        self.n_bin = len(aero_diam_edges) - 1

    def scale(self, factor):
        """Scale the grid by the given factor.

        For example, the two grids below are the same:

        >>> grid_1 = camp.log_grid(5, 10, 100)
        >>> grid_1.scale(3)
        
        >>> grid_2 = camp.log_grid(15, 30, 100)

        """
        self.min = self.min * factor
        self.max = self.max * factor

    def grid_size(self, index, base=10):
        """Return the logarithmic size of the grid bin at the given
        index.

        For a logarithmic grid this will be the same for all bins, and
        is given by the difference of the logarithm of the two bin
        edges in the given base (default base-10).

        Example:
        >>> x_grid = camp.log_grid(1, 100, 10)
        >>> x_grid.grid_size(0)
        0.2

        """
        return (math.log(self.max) / math.log(base)
                - math.log(self.min) / math.log(base)) \
                / float(self.n_bin)

    def valid_bin(self, bin):
        """Whether the given bin number is indeed a valid bin number
        for the grid.

        Example:
        >>> x_grid = camp.log_grid(1, 5, 100)
        >>> x_grid.valid_bin(30)
        True
        >>> x_grid.valid_bin(120)
        False

        """
        if (bin >= 0) and (bin < self.n_bin):
            return True
        return False

    def find(self, values):
        """Return an array of bin indices corresponding to the given
        array of values.

        All of the entries of values must be postive. Note that
        invalid bin indices will be returned if any of the values are
        outside of [min, max] for the grid.

        Example:
        >>> x_grid = camp.log_grid(1, 16, 4)
        >>> x_grid.find(numpy.array([0.3, 3, 8.5, 40]))
        array([-2, 1, 3, 5])

        """
        indices = (numpy.floor((numpy.log(numpy.asarray(values))
                          - math.log(self.min)) * self.n_bin
                         / (math.log(self.max) - math.log(self.min)))
                   ).astype(int)
        return indices

    def edge(self, index):
        """Return the location of the bin edge at the given index.

        The index must be in the range 0 to n_bin, as a grid with
        n_bin grid cells will have (n_bin + 1) edges.

        Example:
        >>> x_grid = camp.log_grid(1, 16, 4)
        >>> x_grid.edge(0)
        1.0
        >>> x_grid.edge(2)
        4.0
        >>> x_grid.edge(4)
        16.0

        """
        if (index < 0) or (index > self.n_bin):
            raise Exception("index out of range: %d" % index)
        if index == self.n_bin:
            return self.max
        elif index == 0:
            return self.min
        else:
            return math.exp(float(index) / float(self.n_bin)
                            * (math.log(self.max) - math.log(self.min))
                            + math.log(self.min))
        
    def center(self, index):
        """Return the location of the bin center at the given
        index.

        The index must be in the range 0 to (n_bin - 1).

        Example:
        >>> x_grid = camp.log_grid(1, 64, 3)
        >>> x_grid.center(0)
        2.0
        >>> x_grid.center(1)
        8.0
        >>> x_grid.center(2)
        32.0

        """
        if (index < 0) or (index >= self.n_bin):
            raise Exception("index out of range: %d" % index)
        return math.exp((float(index) + 0.5) / float(self.n_bin)
                        * (math.log(self.max) - math.log(self.min))
                        + math.log(self.min))

    def half_sample(self):
        """Return a new log_grid object with the same limits but
        half the number of bins as the current grid.

        """
        if self.n_bin % 2 != 0:
            raise Exception("n_bin must be an even number")
        return log_grid(min=self.min, max=self.max,
                            n_bin=self.n_bin / 2)

def histogram_1d(x_values, x_grid, weighted=True, weights=None):
    """Make a 1D histogram.

    The histogram is of points at positions x_values[i] for each i.

    By default, the values are weighted by the inverse of the grid
    sizes. If weighted=False then no weighting is performed.

    Example:
    >>> x_grid = camp.log_grid(min=1e-8, max=1e-5, n_bin=70)
    >>> hist = camp.histogram_1d(diam, x_grid, weights=particles.num_concs)
    >>> plt.semilogx(x_grid.centers(), hist)
    
    """
    if weights is not None:
        if len(x_values) != len(weights):
            raise Exception("x_values and weights have different lengths")
    x_bins = x_grid.find(x_values)
    if weighted:
        hist = numpy.zeros([x_grid.n_bin])
    else:
        hist = numpy.zeros([x_grid.n_bin], dtype=int)
    for i in range(len(x_values)):
        if x_grid.valid_bin(x_bins[i]):
            if weighted:
                value = 1.0 / x_grid.grid_size(x_bins[i])
                if weights is not None:
                    value *= weights[i]
            else:
                value = 1
            hist[x_bins[i]] += value
    return hist

def histogram_2d(x_values, y_values, x_grid, y_grid, weighted=True, weights=None, only_positive=True):
    """Make a 2D histogram.

    The histogram is of points at positions (x_values[i], y_values[i])
    for each i.

    By default, the values are weighted by the inverse of the grid
    sizes. If weighted=False then no weighting is performed.

    Example:
    >>> x_grid = camp.log_grid(min=1e-8, max=1e-5, n_bin=70)
    >>> y_grid = camp.linear_grid(min=0, max=1, n_bin=50)
    >>> hist = camp.histogram_2d(diam, bc_frac, x_grid, y_grid, weights=particles.num_concs)
    >>> plt.pcolor(x_grid.edges(), y_grid.edges(), hist.transpose(),
                   norm=matplotlib.colors.LogNorm(), linewidths=0.1)

    """
    if len(x_values) != len(y_values):
        raise Exception("x_values and y_values have different lengths")
    if weights is not None:
        if len(x_values) != len(weights):
            raise Exception("x_values and weights have different lengths")
    x_bins = x_grid.find(x_values)
    y_bins = y_grid.find(y_values)
    if weighted:
        hist = numpy.zeros([x_grid.n_bin, y_grid.n_bin])
    else:
        hist = numpy.zeros([x_grid.n_bin, y_grid.n_bin], dtype=int)
    for i in range(len(x_values)):
        if x_grid.valid_bin(x_bins[i]) and y_grid.valid_bin(y_bins[i]):
            if weighted:
                value = 1.0 / (x_grid.grid_size(x_bins[i]) * y_grid.grid_size(y_bins[i]))
                if weights is not None:
                    value *= weights[i]
            else:
                value = 1
            hist[x_bins[i], y_bins[i]] += value
    if only_positive:
        hist = numpy.ma.masked_less_equal(hist, 0)
    return hist

def multival_2d(x_values, y_values, z_values, x_grid, y_grid, rand_arrange=True):
    """Make a 2D matrix with 0%/33%/66%/100% percentile values.

    The returned matrix represents z_values[i] at position
    (x_values[i], y_values[i]) for each i.

    Example:
    >>> x_grid = camp.log_grid(min=1e-8, max=1e-5, n_bin=140)
    >>> y_grid = camp.linear_grid(min=0, max=1, n_bin=100)
    >>> vals = camp.multival_2d(diam, bc_frac, h2o, x_grid, y_grid)
    >>> plt.pcolor(x_grid.edges(), y_grid.edges(), vals.transpose(),
                   norm=matplotlib.colors.LogNorm(), linewidths=0.1)

    If there are zeros in the z_values but it desirable to use a
    log-scale for the colors, then the zero values can be filtered out
    as in the following example.

    Example:
    >>> x_grid = camp.log_grid(min=1e-8, max=1e-5, n_bin=140)
    >>> y_grid = camp.linear_grid(min=0, max=1, n_bin=100)
    >>> vals = camp.multival_2d(diam, bc_frac, h2o, x_grid, y_grid)
    >>> vals_pos = np.ma.masked_less_equal(vals, 0)
    >>> vals_zero = np.ma.masked_not_equal(vals, 0)
    >>> plt.pcolor(x_grid.edges(), y_grid.edges(), vals_zero.transpose(),
                   cmap=matplotlib.cm.gray,
                   norm=matplotlib.colors.LogNorm(), linewidths=0.1)
    >>> plt.pcolor(x_grid.edges(), y_grid.edges(), vals_pos.transpose(),
                   cmap=matplotlib.cm.jet, 
                   norm=matplotlib.colors.LogNorm(), linewidths=0.1)

    """
    if len(x_values) != len(y_values):
        raise Exception("x_values and y_values have different lengths")
    if len(x_values) != len(z_values):
        raise Exception("x_values and z_values have different lengths")

    low_x_grid = x_grid.half_sample()
    low_y_grid = y_grid.half_sample()
    x_bins = low_x_grid.find(x_values)
    y_bins = low_y_grid.find(y_values)
    z = [[[] for j in range(low_y_grid.n_bin)]
         for i in range(low_x_grid.n_bin)]
    for i in range(len(x_values)):
        if low_x_grid.valid_bin(x_bins[i]) and low_y_grid.valid_bin(y_bins[i]):
            z[x_bins[i]][y_bins[i]].append(z_values[i])
    for x_bin in range(low_x_grid.n_bin):
        for y_bin in range(low_y_grid.n_bin):
            z[x_bin][y_bin].sort()
    grid = numpy.zeros([x_grid.n_bin, y_grid.n_bin])
    mask = numpy.zeros([x_grid.n_bin, y_grid.n_bin], bool)
    for x_bin in range(low_x_grid.n_bin):
        for y_bin in range(low_y_grid.n_bin):
            if len(z[x_bin][y_bin]) > 0:
                subs = [(0,0),(0,1),(1,0),(1,1)]
                if rand_arrange:
                    random.shuffle(subs)
                (sub_min, sub_max, sub_low, sub_high) = subs
                val_min = min(z[x_bin][y_bin])
                val_max = max(z[x_bin][y_bin])
                val_low = percentile(z[x_bin][y_bin], 0.3333)
                val_high = percentile(z[x_bin][y_bin], 0.6666)
                for (sub, val) in [(sub_min, val_min),
                                   (sub_max, val_max),
                                   (sub_low, val_low),
                                   (sub_high, val_high)]:
                    sub_i, sub_j = sub
                    i = x_bin * 2 + sub_i
                    j = y_bin * 2 + sub_j
                    grid[i,j] = val
                    mask[i,j] = True
    mask = numpy.logical_not(mask)
    vals = numpy.ma.array(grid, mask=mask)
    return vals

def time_of_day_string(env_state, separator=":", resolution="minutes"):
    """Return the current time-of-day in a 24-hour string
    representation.

    The optional resolution parameter can be 'hours', 'minutes', or
    'seconds', to indicate the granularity of the result.

    Example:
    >>> ncf = scipy.io.netcdf.netcdf_file('filename.nc', 'r')
    >>> env_state = camp.env_state_t(ncf)
    >>> camp.time_of_day_string(env_state)
    '14:24'
    
    """
    seconds_past_midnight = env_state.start_time_of_day \
        + env_state.elapsed_time
    return time_of_day_string_from_seconds(seconds_past_midnight,
                                           separator, resolution)

def time_of_day_string_from_seconds(time_seconds, separator=":",
                                    resolution="minutes"):
    """Convert a time-of-day in seconds-past-midnight to a 24-hour
    string representation.

    The optional resolution parameter can be 'hours', 'minutes', or
    'seconds', to indicate the granularity of the result.

    Example:
    >>> camp.time_of_day_string_from_seconds(51858.6)
    '14:24'
    >>> camp.time_of_day_string_from_seconds(51858.6, resolution='seconds')
    '14:24:18'
    
    """
    time_of_day = time_seconds % (24 * 3600.0)
    hours = int(time_of_day / 3600.0)
    minutes = int(time_of_day / 60.0) % 60
    seconds = int(time_of_day) % 60
    if resolution == "hours":
        return "%02d" % hours
    if resolution == "minutes":
        return "%02d%s%02d" % (hours, separator, minutes)
    if resolution == "seconds":
        return "%02d%s%02d%s%02d" % (hours, separator, minutes,
                                     separator, seconds)
    else:
        raise Exception("unknown resolution: %s" % resolution)

def read_history(constructor, directory, filename_pattern,
                 print_progress=False):
    """Read a sequence of NetCDF files, extracting data from each one.

    Each file in the given directory whose name matches the
    regular-expression filename_pattern is opened as a NetCDF
    file. Then the given constructor function is applied to read an
    object from the file. The elapsed_time is also read from the file,
    and a list of pairs [time, object] is returned, sorted by the
    times.

    Example:
    >>> gas_state_history = camp.read_history(gas_state_t,
                                      'out/', 'data_0001_[0-9]{8}.nc')
    >>> time = [t for [t, gs] in gas_state_history]
    >>> o3 = [gs.mixing_ratio('O3') for [t, gs] in gas_state_history]
    >>> plt.plot(time, o3)

    """
    filenames = os.listdir(directory)
    filenames.sort()
    data = []
    filename_re = re.compile(filename_pattern)
    for filename in filenames:
        if filename_re.search(filename):
            if print_progress:
                print(filename)
            netcdf_filename = os.path.join(directory, filename)
            ncf = scipy.io.netcdf.netcdf_file(netcdf_filename, 'r')
            env_state = env_state_t(ncf)
            data.append([env_state.elapsed_time, constructor(ncf)])
            ncf.close()
    data.sort()
    return data

def read_any(constructor, directory, filename_pattern):

    """Read any of a set of NetCDF files, extracting data from it.

    A single one of the NetCDF files in the given directory matching
    the regular-expression filename_pattern is opened as a NetCDF
    file. Then the given constructor is applied to read an object from
    the file, which is returned.

    Example:
    >>> aero_data = camp.read_history(aero_data_t,
                                        'out/', 'data_0001_[0-9]{8}.nc')
    >>> print('species names: ', aero_data.names)

    """
    filenames = os.listdir(directory)
    filename_re = re.compile(filename_pattern)
    for filename in filenames:
        if filename_re.search(filename):
            netcdf_filename = os.path.join(directory, filename)
            ncf = scipy.io.netcdf.netcdf_file(netcdf_filename, 'r')
            data = constructor(ncf)
            ncf.close()
            return data
    raise Exception("no NetCDF file found in %s matching %s"
                    % (directory, filename_pattern))

def get_filename_list(directory, filename_pattern):
    """Return a list of files in a directory matching a given pattern.

    The filename_pattern is a regular expression. All filenames in the
    given directory that match the pattern are returned in a sorted
    list. If the regular expression contains a group then the returned
    list consists of (filename, match) pairs, where the match is the
    group match.

    Example:
    >>> netcdf_files = camp.get_filename_list('out/', r'data_.*\.nc')

    """
    filename_list = []
    filenames = os.listdir(directory)
    if len(filenames) == 0:
        raise Exception("No files in %s match %s"
                        % (directory, filename_pattern))
    file_re = re.compile(filename_pattern)
    for filename in filenames:
        match = file_re.search(filename)
        if match:
            full_filename = os.path.join(directory, filename)
            groups = match.groups()
            if len(groups) > 0:
                filename_list.append((full_filename, groups[0]))
            else:
                filename_list.append(full_filename)
    filename_list.sort()
    if len(filename_list) == 0:
        raise Exception("No files found in %s matching %s"
                        % (directory, filename_pattern))
    return filename_list

def get_time_filename_list(dir, file_pattern):
    """Return a list of files in a directory matching a given pattern
    with times and keys.

    The filename_pattern is a regular expression. All filenames in the
    given directory that match the pattern are returned in a list
    where each entry is of the form [time, filename, key]. The time is
    determined by opening each file as a NetCDF file and reading the
    elapsed_time from it. If the file_pattern contains a regular
    expression group then the key is the value of that group after the
    match, otherwise it is None.

    Example:
    >>> netcdf_files = camp.get_filename_list('out/', r'data_(.*)\.nc')

    """
    time_filename_list = []
    filenames = os.listdir(dir)
    if len(filenames) == 0:
        raise Exception("No files in %s match %s" % (dir, file_pattern))
    file_re = re.compile(file_pattern)
    for filename in filenames:
        match = file_re.search(filename)
        if match:
            groups = match.groups()
            if len(groups) > 0:
                output_key = groups[0]
            else:
                output_key = None
            netcdf_filename = os.path.join(dir, filename)
            ncf = scipy.io.netcdf.netcdf_file(netcdf_filename, 'r')
            env_state = env_state_t(ncf)
            time_filename_list.append([env_state.elapsed_time,
                                       netcdf_filename,
                                       output_key])
            ncf.close()
    time_filename_list.sort()
    if len(time_filename_list) == 0:
        raise Exception("No files found in %s matching %s"
                        % (dir, file_pattern))
    return time_filename_list

def find_nearest_index(data, value):
    """Find the index of the entry in data that is closest to value.

    Example:
    >>> data = [0, 3, 5, -2]
    >>> i = camp.find_nearest_index(data, 3.4)
    returns i = 1

    """
    min_diff = abs(value - data[0])
    min_i = 0
    for i in range(1,len(data)):
        diff = abs(value - data[i])
        if diff < min_diff:
            min_diff = diff
            min_i = i
    return min_i

def argmax(data):
    """Find the index of the largest entry in data.

    If there are multiple equally-large entries then the index of the
    first is returned.

    Example:
    >>> data = [0, 3, 5, -2]
    >>> i = camp.argmax(data)
    returns i = 2

    """
    max_val = data[0]
    val_i = 0
    for i in range(1,len(data)):
        if data[i] > max_val:
            max_val = data[i]
            val_i = i
    return val_i

def argmin(data):
    """Find the index of the smallest entry in data.

    If there are multiple equally-small entries then the index of the
    first is returned.

    Example:
    >>> data = [0, 3, 5, -2]
    >>> i = camp.argminx(data)
    returns i = 3

    """
    min_val = data[0]
    val_i = 0
    for i in range(1,len(data)):
        if data[i] < min_val:
            min_val = data[i]
            val_i = i
    return val_i

def find_nearest_time(time_indexed_data, search_time):
    """Find the closest data-point in a time-sequenced data-set to a
    given time.

    The time_indexed_data argument should be a list of lists of the
    form [[t_0, ...], [t_1, ...], [t_2, ...], ...] where the t_i are
    times. The return value is the index i such that t_i is the
    closest time to search_time. It is not necessary for
    time_indexed_data to be sorted by time.

    Example:
    >>> data = [[0, 50], [3, 45], [5, 40]]
    >>> i = camp.find_nearest_time(data, 3.4)
    returns i = 1
    
    """
    min_diff = abs(search_time - time_indexed_data[0][0])
    min_i = 0
    for i in range(1,len(time_indexed_data)):
        diff = abs(search_time - time_indexed_data[i][0])
        if diff < min_diff:
            min_diff = diff
            min_i = i
    return min_i

def find_filename_at_time(time_filename_list, search_time):
    """Given a time-indexed list of filenames, find the filename
    closest to the given time.

    The argument time_filename_list should be a list of of the form
    [[t_0, f_0, ...], [t_1, f_1, ...], ...] where the t_i are times
    and the f_i are filenames. The return value is the filename f_i
    for which the corresponding time t_i is closest to the search_time
    argument.

    Example:
    >>> netcdf_files = camp.get_filename_list('out/', r'data_(.*)\.nc')
    >>> input_file = camp.find_filename_at_time(netcdf_files, 12 * 3600)
    returns the filename closest to 12 hours elapsed time.

    """
    i = find_nearest_time(time_filename_list, search_time)
    return time_filename_list[i][1]

def cumulative_plot_data(x, y_inc, start=0.0, final=None):
    """Transform density data to cumulative data.

    The x and y_inc arguments should be lists of the same length so
    that (x[i], y_inc[i]) are density points. The return value will be
    two lists (x, y) which are the coordinate of the corresponding
    cumulative distribution.

    The optional argument start can be given to start the plot at a
    non-zero y value. If the optional argument final is not None then
    it specifies the final y value.

    Example:
    >>> (x, y) = camp.cumulative_plot_data(diameter, bc_mass)
    >>> plt.plot(x, y)
    plots the cumulative distribution of BC mass versus diameter.

    """
    plot_data_x = []
    plot_data_y = []
    y = start
    for i in range(x.size):
        plot_data_x.append(x[i])
        plot_data_y.append(y)
        y += y_inc[i]
        if (i == x.size - 1) and (final != None):
            y = final
        plot_data_x.append(x[i])
        plot_data_y.append(y)
    return (plot_data_x, plot_data_y)

def cumulative_hi_res(x, y_inc, start=0.0, final=None,
                      min_x_step=None, min_y_step=None,
                      min_x_factor=None, min_y_factor=None):
    """Transform density data to cumulative data.

    The x and y_inc arguments should be lists of the same length so
    that (x[i], y_inc[i]) are density points. The return value will be
    two lists (x, y) which are the coordinate of the corresponding
    cumulative distribution.

    The optional argument start can be given to start the plot at a
    non-zero y value. If the optional argument final is not None then
    it specifies the final y value.

    The optional arguments min_x_step, min_y_step, min_x_factor, and
    max_x_factor can be provided for high-resolution original data to
    reduced the number of data-points in the computed comulative
    distribution.

    If the optional arguments min_x_step or min_y_step are provided
    then they determine sufficient minimum x and y steps for points to
    be included on the cumulative distribution. Similarly,
    min_x_factor and min_y_factor are sufficient minimum ratios of
    successive x or y values in the cumulative distribution.

    Example:
    >>> (x, y) = camp.cumulative_hi_res(diameter, bc_mass,
                                          min_x_factor=10**0.01)
    >>> plt.plot(x, y)
    
    plots the cumulative distribution of BC mass versus diameter, with
    100 steps per decade on a logarithmic x scale.

    """
    plot_data_x = []
    plot_data_y = []
    i = 0
    y = start
    plot_data_x.append(x[i])
    plot_data_y.append(y)
    last_x, last_y = x[i], y
    for i in range(1,x.size):
        y += y_inc[i]
        if (i == x.size - 1) and (final != None):
            y = final
        if ((min_x_step != None) and (x[i] - last_x > min_x_step)) \
                or ((min_y_step != None) and (y - last_y > min_y_step)) \
                or ((min_x_factor != None) and (x[i]/last_x > min_x_factor)) \
                or ((min_y_factor != None) and (y/last_y > min_y_factor)) \
                or (i == x.size - 1):
            plot_data_x.append(x[i])
            plot_data_y.append(y)
            last_x = x[i]
            last_y = y
    return (plot_data_x, plot_data_y)

def percentile(data, p):
    """Return the data element closest to the given percentile.

    The data argument must be a sorted list. The p argument should be
    in [0, 1] and specifies the percentile rank of the data item to
    return.

    Example:
    >>> camp.percentile([0, 1, 2, 3, 4, 5, 6, 6.1, 6.2, 6.3], 0.9)
    returns 6.2 as being the 90% percentile value in the given data.

    """
    i = int(math.floor((len(data) - 1) * p + 0.5))
    return data[i]

def smooth(x,window_len=10,window='hanning'):
    """smooth the data using a window with requested size.
    
    This method is based on the convolution of a scaled window with the signal.
    The signal is prepared by introducing reflected copies of the signal 
    (with the window size) in both ends so that transient parts are minimized
    in the begining and end part of the output signal.
    
    input:
        x: the input signal 
        window_len: the dimension of the smoothing window
        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'
            flat window will produce a moving average smoothing.

    output:
        the smoothed signal
        
    example:

    t=linspace(-2,2,0.1)
    x=sin(t)+randn(len(t))*0.1
    y=smooth(x)
    
    see also: 
    
    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve
    scipy.signal.lfilter
 
    TODO: the window parameter could be the window itself if an array instead of a string   
    """

    if x.ndim != 1:
        raise ValueError("smooth only accepts 1 dimension arrays.")

    if x.size < window_len:
        raise ValueError("Input vector needs to be bigger than window size.")

    if window_len<3:
        return x

    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:
        raise ValueError("Window is not one of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'")

    s=numpy.r_[2*x[0]-x[window_len:1:-1],x,2*x[-1]-x[-1:-window_len:-1]]
    if window == 'flat':
        w=ones(window_len,'d')
    else:
        w=eval('numpy.'+window+'(window_len)')

    y=numpy.convolve(w/w.sum(),s,mode='same')
    return y[window_len-1:-window_len+1]
